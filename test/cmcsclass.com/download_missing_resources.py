#!/usr/bin/env python3
"""
è‡ªåŠ¨æ£€æµ‹å¹¶ä¸‹è½½ç¼ºå¤±çš„ç½‘ç«™èµ„æº
æ‰«ææ‰€æœ‰ HTML æ–‡ä»¶ï¼Œæå–èµ„æºé“¾æ¥ï¼ˆJSã€CSSã€å›¾ç‰‡ç­‰ï¼‰ï¼Œ
æ£€æŸ¥æœ¬åœ°æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä»åŸç«™ç‚¹ä¸‹è½½
"""

import os
import re
import requests
from pathlib import Path
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, unquote

# é…ç½®
BASE_DIR = Path('.')  # ç½‘ç«™æ ¹ç›®å½•
ORIGINAL_SITE = 'https://macmasterimaritime.com'  # åŸç«™ç‚¹ URL
TIMEOUT = 10  # ä¸‹è½½è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

# éœ€è¦æ£€æŸ¥çš„èµ„æºç±»å‹
RESOURCE_PATTERNS = {
    'script': ['src'],
    'link': ['href'],
    'img': ['src', 'data-src'],
    'source': ['srcset'],
    'video': ['src', 'poster'],
    'audio': ['src'],
}

def extract_resources_from_html(html_path):
    """ä» HTML æ–‡ä»¶ä¸­æå–æ‰€æœ‰èµ„æºé“¾æ¥"""
    resources = set()
    
    try:
        with open(html_path, 'r', encoding='utf-8') as f:
            soup = BeautifulSoup(f.read(), 'html.parser')
        
        # æå–æ ‡ç­¾ä¸­çš„èµ„æº
        for tag_name, attrs in RESOURCE_PATTERNS.items():
            for tag in soup.find_all(tag_name):
                for attr in attrs:
                    value = tag.get(attr)
                    if value:
                        # å¤„ç† srcsetï¼ˆå¯èƒ½åŒ…å«å¤šä¸ª URLï¼‰
                        if attr == 'srcset':
                            urls = re.findall(r'([^\s,]+)', value)
                            resources.update(urls)
                        else:
                            resources.add(value)
        
        # æå– CSS ä¸­çš„ url()
        for style_tag in soup.find_all('style'):
            if style_tag.string:
                urls = re.findall(r'url\(["\']?([^"\')]+)["\']?\)', style_tag.string)
                resources.update(urls)
        
        # æå–å†…è” style å±æ€§ä¸­çš„ url()
        for tag in soup.find_all(style=True):
            urls = re.findall(r'url\(["\']?([^"\')]+)["\']?\)', tag['style'])
            resources.update(urls)
    
    except Exception as e:
        print(f"âš ï¸  è§£æ {html_path} å¤±è´¥: {e}")
    
    return resources

def normalize_path(resource_url, html_path):
    """å°†èµ„æº URL è½¬æ¢ä¸ºæœ¬åœ°æ–‡ä»¶è·¯å¾„"""
    # ç§»é™¤æŸ¥è¯¢å‚æ•°å’Œé”šç‚¹
    resource_url = resource_url.split('?')[0].split('#')[0]
    
    # è·³è¿‡å¤–éƒ¨é“¾æ¥å’Œç‰¹æ®Šåè®®
    if resource_url.startswith(('http://', 'https://', '//', 'data:', 'javascript:', 'mailto:')):
        return None
    
    # å¤„ç†ç»å¯¹è·¯å¾„ï¼ˆä»¥ / å¼€å¤´ï¼‰
    if resource_url.startswith('/'):
        return BASE_DIR / resource_url.lstrip('/')
    
    # å¤„ç†ç›¸å¯¹è·¯å¾„
    html_dir = Path(html_path).parent
    resource_path = html_dir / resource_url
    
    # è§„èŒƒåŒ–è·¯å¾„ï¼ˆè§£æ ../ ç­‰ï¼‰
    try:
        return resource_path.resolve()
    except:
        return None

def download_resource(local_path, original_url):
    """ä»åŸç«™ç‚¹ä¸‹è½½èµ„æºå¹¶ä¿å­˜åˆ°æœ¬åœ°"""
    try:
        # åˆ›å»ºç›®å½•
        local_path.parent.mkdir(parents=True, exist_ok=True)
        
        # ä¸‹è½½æ–‡ä»¶
        print(f"â¬‡ï¸  ä¸‹è½½: {original_url}")
        response = requests.get(original_url, timeout=TIMEOUT, stream=True)
        response.raise_for_status()
        
        # ä¿å­˜æ–‡ä»¶
        with open(local_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        
        print(f"âœ…  ä¿å­˜åˆ°: {local_path}")
        return True
    
    except Exception as e:
        print(f"âŒ  ä¸‹è½½å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("ğŸ” å¼€å§‹æ‰«æç½‘ç«™èµ„æº...")
    print("=" * 80)
    
    all_resources = {}  # {local_path: original_url}
    missing_resources = {}
    
    # 1. æ‰«ææ‰€æœ‰ HTML æ–‡ä»¶
    html_files = list(BASE_DIR.rglob('*.html'))
    print(f"\nğŸ“„ æ‰¾åˆ° {len(html_files)} ä¸ª HTML æ–‡ä»¶\n")
    
    for html_file in html_files:
        print(f"ğŸ“ æ‰«æ: {html_file.relative_to(BASE_DIR)}")
        resources = extract_resources_from_html(html_file)
        
        for resource_url in resources:
            local_path = normalize_path(resource_url, html_file)
            
            if local_path and local_path != BASE_DIR:
                # æ„é€ åŸç«™ç‚¹ URL
                try:
                    # Ensure both are absolute
                    abs_local = local_path.resolve()
                    abs_base = BASE_DIR.resolve()
                    if abs_local.is_relative_to(abs_base):
                        relative_to_base = abs_local.relative_to(abs_base)
                        original_url = f"{ORIGINAL_SITE}/{str(relative_to_base).replace(os.sep, '/')}"
                        all_resources[local_path] = original_url
                except Exception as e:
                    pass
    
    print(f"\nâœ… æ€»å…±æ‰¾åˆ° {len(all_resources)} ä¸ªèµ„æºå¼•ç”¨\n")
    
    # 2. æ£€æŸ¥å“ªäº›èµ„æºæœ¬åœ°ä¸å­˜åœ¨
    print("=" * 80)
    print("ğŸ” æ£€æŸ¥ç¼ºå¤±çš„èµ„æº...")
    print("=" * 80 + "\n")
    
    for local_path, original_url in all_resources.items():
        if not local_path.exists():
            missing_resources[local_path] = original_url
            print(f"âŒ  ç¼ºå¤±: {local_path.relative_to(BASE_DIR)}")
    
    if not missing_resources:
        print("ğŸ‰ æ‰€æœ‰èµ„æºéƒ½å­˜åœ¨ï¼Œæ— éœ€ä¸‹è½½ï¼")
        return
    
    print(f"\nâš ï¸  å‘ç° {len(missing_resources)} ä¸ªç¼ºå¤±çš„èµ„æº\n")
    
    # 3. è¯¢é—®æ˜¯å¦ä¸‹è½½
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--yes', '-y', action='store_true', help='Automatically confirm download')
    args, unknown = parser.parse_known_args()

    if not args.yes:
        response = input("æ˜¯å¦ä»åŸç«™ç‚¹ä¸‹è½½è¿™äº›èµ„æºï¼Ÿ(y/n): ")
        if response.lower() != 'y':
            print("å·²å–æ¶ˆä¸‹è½½")
            return
    else:
        print("å·²è‡ªåŠ¨ç¡®è®¤ä¸‹è½½ (--yes)")
    
    # 4. ä¸‹è½½ç¼ºå¤±çš„èµ„æº
    print("\n" + "=" * 80)
    print("â¬‡ï¸  å¼€å§‹ä¸‹è½½ç¼ºå¤±çš„èµ„æº...")
    print("=" * 80 + "\n")
    
    success_count = 0
    failed_count = 0
    
    for local_path, original_url in missing_resources.items():
        if download_resource(local_path, original_url):
            success_count += 1
        else:
            failed_count += 1
        print()
    
    # 5. æ€»ç»“
    print("=" * 80)
    print("ğŸ“Š ä¸‹è½½å®Œæˆï¼")
    print("=" * 80)
    print(f"âœ… æˆåŠŸ: {success_count}")
    print(f"âŒ å¤±è´¥: {failed_count}")
    print(f"ğŸ“¦ æ€»è®¡: {len(missing_resources)}")

if __name__ == '__main__':
    main()
